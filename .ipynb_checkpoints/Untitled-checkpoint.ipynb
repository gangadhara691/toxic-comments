{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from backports import csv\n",
    "import numpy as np\n",
    "# Helps in reading long texts\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "\n",
    "def get_texts_and_targets(filename):\n",
    "    texts = []\n",
    "    targets = []\n",
    "\n",
    "    with io.open(filename, encoding='utf-8') as csvfile:\n",
    "        readCSV = csv.reader(csvfile)\n",
    "        for i, row in enumerate(readCSV):\n",
    "            if i == 0:\n",
    "                # Header row\n",
    "                continue\n",
    "            texts.append(row[1].strip().encode('ascii', 'replace'))\n",
    "            targets.append(np.array([float(x) for x in row[2:]]))\n",
    "    print(\"Total number of texts: %s\" % len(texts))\n",
    "    return texts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Max number of input words in any sample\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "VALIDATION_SPLIT = 0.1\n",
    "def get_datasets(texts, targets, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    targets = np.asarray(targets)\n",
    "\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    targets = targets[indices]\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "    x_train = data[:-nb_validation_samples]\n",
    "    y_train = targets[:-nb_validation_samples]\n",
    "    x_val = data[-nb_validation_samples:]\n",
    "    y_val = targets[-nb_validation_samples:]\n",
    "\n",
    "    return tokenizer, word_index, x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "def load_glove_model():\n",
    "    word2vec = KeyedVectors.load_word2vec_format(\n",
    "            os.path.join(WORD2VEC_FOLDER,\n",
    "                'word2vec_twitter_glove.txt'),\n",
    "            binary=False)\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_layer(word_index, gensim_model):\n",
    "    embedding_dim = len(gensim_model.wv['apple'])\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in gensim_model.wv.vocab:\n",
    "            embedding_matrix[i] = gensim_model.wv[word]\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "            embedding_dim,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=True)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "N_TARGET_CLASSES = 6\n",
    "def get_convnet_model(embedding_layer):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    preds = Dense(N_TARGET_CLASSES, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts, targets = get_texts_and_targets('train.csv')\n",
    "tokenizer, word_index, x_train, y_train, x_val, y_val = get_datasets(texts, targets)\n",
    "word2vec = load_word2vec_model()\n",
    "embedding_layer = get_embedding_layer(word_index, word2vec)\n",
    "model = get_convnet_model(embedding_layer)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adagrad',\n",
    "        metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2, batch_size=32, verbose=1)\n",
    "\n",
    "score = model.evaluate(x_val, y_val,\n",
    "                       batch_size = batch_size)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
